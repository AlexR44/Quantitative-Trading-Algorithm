{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9403d712-5fba-4444-8a08-16cdae8fe9c5",
   "metadata": {},
   "source": [
    "# Back Testing Fuction\n",
    "For a variation of portfolio asset weights (input), the following function calculates the following:\n",
    "* Beta\n",
    "* Alpha\n",
    "* Sharpe Ratio\n",
    "* Sortino Ratio\n",
    "* Drawdown\n",
    "* Value at Risk (VaR)\n",
    "* Conditional Value at Risk (cVaR)\n",
    "* Risk Contribution\n",
    "\n",
    "It returns the various metrics to aid in the portfoio optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "635a2ac6-a643-456b-97aa-19079e61c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest (weights, ret_df, bench, timeframe=252, plots=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    weights = 1d numpy array\n",
    "    ret_df = portfolio dataframe with daily asset returns\n",
    "    bench = string type with name of the market benchmark\n",
    "    timeframe = int type annualization factor\n",
    "    \"\"\"\n",
    "    # Importing required libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime as dt\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "    import hvplot.pandas\n",
    "    import sympy as sym\n",
    "    \n",
    "    ############### Computing portfolio based on weights input ###############\n",
    "    portfolio_df=np.multiply(ret_df,np.transpose(weights))\n",
    "    portfolio_df=portfolio_df.sum(axis=1)\n",
    "    portfolio_df=pd.DataFrame(portfolio_df)\n",
    "    portfolio_df.columns=['portfolio']\n",
    "    columns = ret_df.columns\n",
    "    columns = [col for col in columns]\n",
    "    \n",
    "    ############### Computing BETA #############################################\n",
    "    %run bench_stock_price.ipynb\n",
    "    \n",
    "    # pulling data for benchmark\n",
    "    from pandas.tseries.offsets import DateOffset\n",
    "    from_date=portfolio_df.index[0]-DateOffset(days=1)\n",
    "    from_date=from_date.strftime(\"%Y-%m-%d\")\n",
    "    today=portfolio_df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "    bench=bench_stock_data('^GSPC',from_date,today)\n",
    "    bench=pd.DataFrame(bench)\n",
    "    bench.columns=['date','open','high','low','close','volume','adjClose']\n",
    "    bench=bench.set_index('date').sort_index(axis=0, ascending=True)\n",
    "    bench=bench['adjClose'].pct_change().dropna()\n",
    "    bench=pd.DataFrame(bench)\n",
    "    bench.columns=['benchmark']\n",
    "    bench.index=pd.to_datetime(bench.index, errors='raise',yearfirst=True, infer_datetime_format=True)\n",
    "    \n",
    "    # Concatenating portfolio and bench into a single df\n",
    "    join = pd.concat([portfolio_df,bench],axis=1)\n",
    "    \n",
    "    # Calculating covariance of portfolio and bench\n",
    "    cov = np.cov(join, rowvar=False)[0][1]\n",
    "    \n",
    "    # Calculating variance of bench\n",
    "    var = np.cov(join, rowvar=False)[1][1]\n",
    "    \n",
    "    # Calculating BETA\n",
    "    beta = cov/var\n",
    "    \n",
    "    ############### Computing ALPHA #############################################\n",
    "   # Calculating mean of returns for the portfolio\n",
    "    mean_ret = join.mean()*timeframe\n",
    "    mean_portfolio_ret=mean_ret['portfolio']\n",
    "    # Calculating mean of returns for the bench\n",
    "    mean_bench_ret = mean_ret['benchmark']\n",
    "    \n",
    "    # Calculating ALPHA\n",
    "    alpha = mean_portfolio_ret/mean_bench_ret\n",
    "    alpha\n",
    "    \n",
    "    \n",
    "     ############### Computing SHARPE RATIO #############################################\n",
    "    # Setting up covariance matrix(C), mean returns (m) for assets in portfolio to solve for MVP analytically\n",
    "    w=sym.Matrix(weights).transpose()\n",
    "    C=sym.Matrix((ret_df.cov()).values)\n",
    "    m=sym.Matrix(ret_df.mean().values)\n",
    "    m=m.transpose()\n",
    "    u=[]\n",
    "    \n",
    "    # calculating portfolio mean returns and variance\n",
    "    mu_v=w*m.transpose()\n",
    "    sigma_sq=(w*C)*w.transpose()\n",
    "    \n",
    "    mu_v=float(mu_v.col(0)[:][0])*timeframe  \n",
    "    sigma_sq=float(sigma_sq.col(0)[:][0])\n",
    "    sigma_sym=np.sqrt(sigma_sq)*np.sqrt(252)\n",
    "    \n",
    "    mean = portfolio_df.mean()*timeframe\n",
    "    std = portfolio_df.std()*np.sqrt(252)\n",
    "    \n",
    "    #sharpe_1 = mean[0]/std[0]\n",
    "    sharpe=mu_v/sigma_sym\n",
    "     ############### Computing SORTINO RATIO #############################################\n",
    "    down = portfolio_df[portfolio_df < 0]\n",
    "    std_down = down.std()*np.sqrt(timeframe)\n",
    "    \n",
    "    sortino = mean/std_down\n",
    "    sortino=sortino[0]\n",
    "    \n",
    "    ############### Computing DRAWDOWN #############################################\n",
    "    # Calculating cumulative returns of portfolio\n",
    "    cum_rets = (portfolio_df+1).cumprod()\n",
    "    \n",
    "    # Calculating the running max\n",
    "    running_max = np.maximum. accumulate(cum_rets.dropna())\n",
    "    running_max[running_max < 1] = 1\n",
    "    \n",
    "    \n",
    "    # Calculating DRAWDOWN\n",
    "    drawdown = cum_rets/running_max-1\n",
    "    min_drawdown = -drawdown.min()\n",
    "    min_drawdown=min_drawdown[0]\n",
    "    \n",
    "     ############### Computing VaR #############################################\n",
    "    \n",
    "    theta = 0.05 #----> \n",
    "    n = 100000 #----->number of simulations to run\n",
    "    \n",
    "    # Calculating the values for theta error threshold\n",
    "    t = int(n*theta)\n",
    "    \n",
    "    # Creating vector with n simulation with a normal law\n",
    "    vector = pd.DataFrame( np.random.normal(mean, std, size=(n,)), columns= ['simulations'])\n",
    "    \n",
    "    # Ordering values and finding the theta value\n",
    "    var = vector.sort_values(by = 'simulations').iloc[t].values[0]\n",
    "    \n",
    "    \n",
    "    ############### Computing cVaR #############################################\n",
    "    cvar = -vector.sort_values(by='simulations').iloc[0:t,:].mean()\n",
    "    cvar=cvar[0]\n",
    "    ############### Computing Risk contribution #############################################\n",
    "    %run retrieving_data_from_db.ipynb    \n",
    "    num_assets = len(weights)  # number of assets in portfolio\n",
    "    \n",
    "    # Calculating risk contribution for each asset\n",
    "    beta2=[]\n",
    "    tickers = ret_df.columns\n",
    "\n",
    "    # Bench data \n",
    "    from pandas.tseries.offsets import DateOffset\n",
    "    from_date=ret_df.index[0]-DateOffset(days=1)\n",
    "    from_date=from_date.strftime(\"%Y-%m-%d\")\n",
    "    today=df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "    bench2=bench_stock_data('^GSPC',from_date,today)\n",
    "    bench2=pd.DataFrame(bench2)\n",
    "    bench2.columns=['date','open','high','low','close','volume','adjClose']\n",
    "    bench2=bench2.set_index('date').sort_index(axis=0, ascending=True)\n",
    "    bench2=bench2['adjClose'].pct_change().dropna()\n",
    "    bench2=pd.DataFrame(bench2)\n",
    "    bench2.columns=['benchmark']\n",
    "    bench2.index=pd.to_datetime(bench2.index, errors='raise',yearfirst=True, infer_datetime_format=True)\n",
    "    cumulative_benchmark=(bench2).cumsum()\n",
    "    bench2_mean=bench2.mean()\n",
    "    bench2_std=bench2.std()\n",
    "    \n",
    "    for i in columns:\n",
    "        try:      \n",
    "            # Stock data\n",
    "            a=ret_df[i]    \n",
    "            # Concatenating asset data and bench data into a single df\n",
    "            join2 = pd. concat((a,bench2), axis=1)\n",
    "        \n",
    "            # Calculating covariance of portfolio and bench\n",
    "            cov2 = np.cov(join2, rowvar=False)[0][1]\n",
    "        \n",
    "            # Calculating variance of bench\n",
    "            var2 = np.cov(join2, rowvar=False)[1][1]\n",
    "        \n",
    "            # Calculating BETA\n",
    "            beta2.append(cov2/var2)      \n",
    "            \n",
    "        except:\n",
    "            pass     \n",
    "            \n",
    "    # Individual risk \n",
    "    r = np.multiply(np.abs(beta2),np.transpose(weights))\n",
    "    # Risk contribution\n",
    "    cr = r/(np.sum(r))*100\n",
    "    cr=pd.DataFrame(cr)\n",
    "    tickers=pd.DataFrame(columns)\n",
    "    cr=pd. concat((tickers,cr), axis=1) \n",
    "    cr.columns=['ticker','rc']  \n",
    "    cr=cr.set_index('ticker')\n",
    "    \n",
    "    ############### Plotting Results #############################################\n",
    "    \n",
    "    if plots == True:\n",
    "        \n",
    "        display((portfolio_df.cumsum().hvplot(title='Cumulative Results',label='Portfolio')*(\n",
    "                    cumulative_benchmark.hvplot(color='black',label='Benchmark',ylabel='Cumulative Ret.'))).opts(legend_position='top_left'))\n",
    "        display(drawdown.hvplot.area(color='red',ylabel='Drawdown'))\n",
    "        \n",
    "        \n",
    "    \n",
    "    return [beta,sharpe,sortino,min_drawdown,var,mu_v,sigma_sym]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c0f27-1c83-426a-9bef-e0ef46d80bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04535645-b222-4d71-aece-15e2d28e287d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68dcbaf0-8006-4106-970f-fe752828462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from pathlib import Path \\nimport pandas as pd\\nfilepath = Path('./ss.csv')\\nstock_selection_df=pd.read_csv(filepath)\\nstock_selection_df=stock_selection_df.drop_duplicates()\\nstock_selection_df['price_date']=pd.to_datetime(stock_selection_df['price_date'])\\nstock_selection_df=stock_selection_df.set_index('price_date')\\nstock_selection_df=stock_selection_df.drop_duplicates()\\nstock_selection_df\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#USE TO TROUBLESHOOT\n",
    "\"\"\"from pathlib import Path \n",
    "import pandas as pd\n",
    "filepath = Path('./ss.csv')\n",
    "stock_selection_df=pd.read_csv(filepath)\n",
    "stock_selection_df=stock_selection_df.drop_duplicates()\n",
    "stock_selection_df['price_date']=pd.to_datetime(stock_selection_df['price_date'])\n",
    "stock_selection_df=stock_selection_df.set_index('price_date')\n",
    "stock_selection_df=stock_selection_df.drop_duplicates()\n",
    "stock_selection_df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c38e807-d79e-42ff-a581-b3a07adafe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"weights=[]\\nfor i in stock_selection_df.columns:\\n    #weights.append(1/len(stock_selection_df.columns))\\n#metrics_df=backtest(weights, stock_selection_df, '^GSPC', timeframe=252, plots=True)\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"weights=[]\n",
    "for i in stock_selection_df.columns:\n",
    "    #weights.append(1/len(stock_selection_df.columns))\n",
    "#metrics_df=backtest(weights, stock_selection_df, '^GSPC', timeframe=252, plots=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfb15c5d-187a-4d3e-9d8a-613cf4a7bb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stock_selection_df.index.min()'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"stock_selection_df.index.min()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd2b4cda-fd48-4a35-aa7a-b1b515dd9b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from pandas.tseries.offsets import DateOffset\\nfrom_date=stock_selection_df.index[0]-DateOffset(days=1)\\nfrom_date=from_date.strftime(\"%Y-%m-%d\")\\ntoday=stock_selection_df.index[-1].strftime(\"%Y-%m-%d\")\\nbench2=bench_stock_data(\\'^GSPC\\',from_date,today)\\nbench2=pd.DataFrame(bench2)\\nbench2.columns=[\\'date\\',\\'open\\',\\'high\\',\\'low\\',\\'close\\',\\'volume\\',\\'adjClose\\']\\nbench2=bench2.set_index(\\'date\\').sort_index(axis=0, ascending=True)\\nbench2=bench2.drop_duplicates()\\nbench2'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bench data \n",
    "\"\"\"from pandas.tseries.offsets import DateOffset\n",
    "from_date=stock_selection_df.index[0]-DateOffset(days=1)\n",
    "from_date=from_date.strftime(\"%Y-%m-%d\")\n",
    "today=stock_selection_df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "bench2=bench_stock_data('^GSPC',from_date,today)\n",
    "bench2=pd.DataFrame(bench2)\n",
    "bench2.columns=['date','open','high','low','close','volume','adjClose']\n",
    "bench2=bench2.set_index('date').sort_index(axis=0, ascending=True)\n",
    "bench2=bench2.drop_duplicates()\n",
    "bench2\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f604c6a-7976-4077-9b49-83e8ba3f64bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bench2=bench2['adjClose'].pct_change().dropna()\\nbench2=pd.DataFrame(bench2)\\nbench2.columns=['benchmark']\\nbench2\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"bench2=bench2['adjClose'].pct_change().dropna()\n",
    "bench2=pd.DataFrame(bench2)\n",
    "bench2.columns=['benchmark']\n",
    "bench2\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5d0f724-080f-4aa8-bc62-423296929da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bench2.index=pd.to_datetime(bench2.index, errors='raise',yearfirst=True, infer_datetime_format=True)\\nbench2.info()\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"bench2.index=pd.to_datetime(bench2.index, errors='raise',yearfirst=True, infer_datetime_format=True)\n",
    "bench2.info()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3419f0c-e793-4e58-a9e8-53551e5a7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating portfolio and bench into a single df\n",
    "\"\"\"join = pd.concat([stock_selection_df,bench2],join ='inner',axis=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7644342-f7b6-4cc5-960c-7f597afc605a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
